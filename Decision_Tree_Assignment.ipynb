{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e7fe645-5e10-48b2-a98c-841ed1ecb56e",
   "metadata": {},
   "source": [
    "***Decision Tree | Assignment***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8475b55-cf57-491c-b65f-15d4fcaede90",
   "metadata": {},
   "source": [
    "# Question 1:  What is a Decision Tree, and how does it work in the context of classification? \n",
    "Answer:  \n",
    "- def :  \n",
    "A Decision Tree is a machine learning algorithm that is used for classification and regression tasks. In classification, it is used to predict the class (category) of a given input based on its features.\n",
    "\n",
    "- It works like a flowchart:\n",
    "\n",
    "Each internal node represents a condition on a feature (example: \"Is age > 18?\").\n",
    "\n",
    "Each branch represents the outcome of that condition (Yes/No).\n",
    "\n",
    "Each leaf node gives the final decision or class label.\n",
    "\n",
    "- Working in classification:\n",
    "\n",
    "The dataset is divided into smaller subsets based on the most important feature using measures like Gini Index or Information Gain.\n",
    "\n",
    "This splitting continues until all the data is perfectly classified or some stopping condition is reached.\n",
    "\n",
    "To classify a new example, the tree is followed from the root node to a leaf node by answering the conditions step by step.\n",
    "\n",
    "The class label of the reached leaf node is given as the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777746ec-2375-4082-9208-041c1f9154ce",
   "metadata": {},
   "source": [
    "# Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree? \n",
    "Answer:\n",
    "In a Decision Tree, we need to decide where to split the data. To do this, we measure how ‚Äúimpure‚Äù or ‚Äúmixed‚Äù the classes are in a node. Two common impurity measures are Gini Impurity and Entropy.\n",
    "\n",
    "- Gini Impurity\n",
    "\n",
    "Formula: \n",
    "Gini=1‚àí‚àëpi2‚Äã  \n",
    "here\n",
    "pi= probability of class i in the node.\n",
    "\n",
    "Gini tells us how often a randomly chosen element would be misclassified if we label it randomly according to the class distribution.\n",
    "\n",
    "Range: 0 (pure, only one class) to 0.5 (for 2 classes, completely mixed).\n",
    "\n",
    "Decision Trees (like CART) often use Gini because it is faster to calculate.\n",
    "\n",
    "- Entropy (Information Gain)\n",
    "\n",
    "Formula: \n",
    "Entropy=‚àí‚àëpi‚Äãlog2‚Äã(pi)\n",
    "Measures the uncertainty in the node.\n",
    "\n",
    "Range: 0 (pure) to 1 (for 2 classes equally mixed).\n",
    "\n",
    "When splitting, Decision Trees calculate Information Gain, which is the reduction in entropy after the split.\n",
    "\n",
    "- Impact on splits:\n",
    "\n",
    "Both Gini and Entropy try to create ‚Äúpure‚Äù child nodes (nodes with mostly one class).\n",
    "\n",
    "Gini tends to make splits that isolate the most frequent class quickly.\n",
    "\n",
    "Entropy is more sensitive to class distribution and can give slightly different splits.\n",
    "\n",
    "In practice, both often give similar results, but Gini is computationally simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fab406-17ac-4b43-96ce-5bab4e4159a1",
   "metadata": {},
   "source": [
    "# Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each. \n",
    "Answer: \n",
    "| **Aspect**       | **Pre-Pruning (Early Stopping)**                      | **Post-Pruning**                                    |\n",
    "| ---------------- | ----------------------------------------------------- | --------------------------------------------------- |\n",
    "| **When applied** | During tree building (stops splitting early)          | After the tree is fully grown                       |\n",
    "| **How it works** | Uses conditions like max depth, min samples, min gain | Removes branches that do not improve accuracy       |\n",
    "| **Goal**         | Prevent the tree from becoming too complex            | Simplify a fully grown tree without losing accuracy |\n",
    "| **Advantage**    | Saves time & computation, avoids very large trees     | Usually gives better accuracy and generalization    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a780fed-959e-4912-b10b-6f563c64bdd0",
   "metadata": {},
   "source": [
    "# Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split? \n",
    "Answer:\n",
    "Information Gain (IG):\n",
    "\n",
    "Information Gain measures the reduction in uncertainty (entropy) after splitting the data on an attribute.\n",
    "\n",
    "Formula:\n",
    "\n",
    "IG(S,A)=Entropy(S)‚àí‚àë‚à£S‚à£‚à£Sv‚Äã‚à£‚Äã√óEntropy(Sv‚Äã)\n",
    "\n",
    "where\n",
    "S is the dataset, and ùëÜùë£are the subset after sliting an attribute A\n",
    "\n",
    "- Meaning:\n",
    "\n",
    "High IG means the split gives more ‚Äúpure‚Äù subsets (less mixed classes).\n",
    "\n",
    "Low IG means the split does not help much in separating classes.\n",
    "\n",
    "- Importance in Decision Trees:\n",
    "\n",
    "Decision Trees use IG to choose the best attribute for splitting at each node.\n",
    "\n",
    "The attribute with the highest Information Gain is selected because it reduces uncertainty the most.\n",
    "\n",
    "This helps the tree make more accurate decisions and avoid useless splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f7d8e-fcdc-4d11-8a85-c71b9fead1fb",
   "metadata": {},
   "source": [
    "# Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations? \n",
    "Answer:\n",
    "1. Real-world applications of Decision Trees:\n",
    "\n",
    "Application\tExample\n",
    "Medical Diagnosis e.g:\tPredicting if a patient has a disease\n",
    "Credit Risk / Loan Approval e,g :\tApproving or rejecting loan applications\n",
    "Customer Churn Prediction\te,g:Predicting if a customer will leave a service\n",
    "Spam Email Detection\te,g:Classifying emails as spam or not spam\n",
    "Marketing & Sales\tDeciding which customers to target for a campaign\n",
    "\n",
    "2. Main Advantages:\n",
    "\n",
    "Easy to understand and interpret (like a flowchart)\n",
    "\n",
    "Can handle both numerical and categorical data\n",
    "\n",
    "No need for much data preprocessing\n",
    "\n",
    "Can reveal important features automatically\n",
    "\n",
    "3. Main Limitations:\n",
    "\n",
    "Can overfit if the tree is too deep\n",
    "\n",
    "Sensitive to small changes in data\n",
    "\n",
    "Not always the most accurate compared to ensemble methods (like Random Forest or XGBoost)\n",
    "\n",
    "Can be biased if some classes dominate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2125ca40-1ec7-413f-b8b5-759a83ad3a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n",
      "\n",
      "Feature Importances:\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0191\n",
      "petal length (cm): 0.8933\n",
      "petal width (cm): 0.0876\n"
     ]
    }
   ],
   "source": [
    "#Question 6:   Write a Python program to: ‚óè Load the Iris Dataset ‚óè Train a Decision Tree Classifier using the Gini criterion ‚óè Print the model‚Äôs accuracy and feature importances \n",
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data   # Features\n",
    "y = iris.target # Labels\n",
    "\n",
    "# 2. Split dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Train Decision Tree Classifier with Gini criterion\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# 5. Model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# 6. Feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
    "    print(f\"{name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c0b05e-0fad-49b6-aa1b-31ebd4ad5570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree with max_depth=3: 1.0\n",
      "Accuracy of fully-grown Decision Tree: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Question 7:Write a Python program to: ‚óè Load the Iris Dataset ‚óè Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree. \n",
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2. Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Train a Decision Tree with max_depth=3\n",
    "tree_limited = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
    "tree_limited.fit(X_train, y_train)\n",
    "y_pred_limited = tree_limited.predict(X_test)\n",
    "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
    "\n",
    "# 4. Train a fully-grown Decision Tree (no max_depth)\n",
    "tree_full = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
    "tree_full.fit(X_train, y_train)\n",
    "y_pred_full = tree_full.predict(X_test)\n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "# 5. Print accuracies\n",
    "print(\"Accuracy of Decision Tree with max_depth=3:\", accuracy_limited)\n",
    "print(\"Accuracy of fully-grown Decision Tree:\", accuracy_full)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5042e04a-bdef-4347-9666-790ffe8aa3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5280096503174904\n",
      "\n",
      "Feature Importances:\n",
      "MedInc: 0.5235\n",
      "HouseAge: 0.0521\n",
      "AveRooms: 0.0494\n",
      "AveBedrms: 0.0250\n",
      "Population: 0.0322\n",
      "AveOccup: 0.1390\n",
      "Latitude: 0.0900\n",
      "Longitude: 0.0888\n"
     ]
    }
   ],
   "source": [
    "# Question 8: Write a Python program to: ‚óè Load the Boston Housing Dataset ‚óè Train a Decision Tree Regressor ‚óè Print the Mean Squared Error (MSE) and feature importances \n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Load California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# 2. Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Train Decision Tree Regressor\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predictions\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# 5. Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# 6. Feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for name, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
    "    print(f\"{name}: {importance:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7af5277-9773-44ad-92b0-37cf8b219287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
      "Accuracy of the tuned model: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Question 9: Write a Python program to: ‚óè Load the Iris Dataset ‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using GridSearchCV ‚óè Print the best parameters and the resulting model accuracy \n",
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2. Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Define Decision Tree and parameter grid\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# 4. Apply GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 5. Best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# 6. Evaluate best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the tuned model:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930feb90-ca43-4b50-86e7-700fbc57f375",
   "metadata": {},
   "source": [
    "# Queation 10:Imagine you‚Äôre working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. \n",
    "#Explain the step-by-step process you would follow to: ‚óè Handle the missing values ‚óè Encode the categorical features ‚óè Train a Decision Tree model ‚óè Tune its hyperparameters ‚óè Evaluate its performance \n",
    "#And describe what business value this model could provide in the real-world setting. \n",
    "- Step 1: Handle Missing Values\n",
    "\n",
    "Identify missing values in the dataset using .isnull() or .info().\n",
    "\n",
    "Impute missing values:\n",
    "\n",
    "For numerical features, use mean or median.\n",
    "\n",
    "For categorical features, use mode (most frequent value) or a special category like ‚ÄúUnknown‚Äù.\n",
    "\n",
    "Optional: Drop rows or columns if too many values are missing.\n",
    "\n",
    "- Step 2: Encode Categorical Features\n",
    "\n",
    "Convert categorical variables into numbers because Decision Trees in most libraries require numeric input.\n",
    "\n",
    "Use Label Encoding if the feature is ordinal (has order).\n",
    "\n",
    "Use One-Hot Encoding if the feature is nominal (no order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3125de4c-3066-4396-b43f-498f73a25ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example:\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56d01b-0347-49ce-862f-3fbcaa10b908",
   "metadata": {},
   "source": [
    "Step 3: Train a Decision Tree Model\n",
    "\n",
    "Split the dataset into train and test sets using train_test_split.\n",
    "\n",
    "Train a Decision Tree Classifier on the training data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d30bf-cded-4070-bb08-6e42e0f67b69",
   "metadata": {},
   "source": [
    "- Step 4: Tune Hyperparameters\n",
    "\n",
    "Use GridSearchCV or RandomizedSearchCV to find the best settings:\n",
    "\n",
    "max_depth ‚Üí controls tree depth\n",
    "\n",
    "min_samples_split ‚Üí min samples to split a node\n",
    "\n",
    "min_samples_leaf ‚Üí min samples at a leaf\n",
    "\n",
    "criterion ‚Üí ‚Äúgini‚Äù or ‚Äúentropy‚Äù\n",
    "\n",
    "This prevents overfitting and improves model performance.\n",
    "\n",
    "- Step 5: Evaluate Performance\n",
    "\n",
    "Use the test set to check accuracy.\n",
    "\n",
    "Use additional metrics like:\n",
    "\n",
    "Precision ‚Üí correct positive predictions / all predicted positive\n",
    "\n",
    "Recall ‚Üí correct positive predictions / all actual positive\n",
    "\n",
    "F1-score ‚Üí balance of precision and recall\n",
    "\n",
    "Confusion matrix ‚Üí shows true/false positives/negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8862b1e-f1cf-4625-a6a8-fe617acc8fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "[[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#example:\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bda596-0b63-480b-a1b1-5c7753461ca7",
   "metadata": {},
   "source": [
    "- Step 6: Business Value\n",
    "\n",
    "Early disease detection ‚Üí helps doctors take timely actions.\n",
    "\n",
    "Resource optimization ‚Üí prioritize patients at higher risk.\n",
    "\n",
    "Reduce healthcare costs ‚Üí prevent serious complications by early intervention.\n",
    "\n",
    "Personalized care ‚Üí target treatment based on predicted risk.\n",
    "\n",
    "This model can save lives, improve patient care, and reduce costs by identifying high-risk patients before the disease progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52e2d0-03af-4fc5-93c8-0e0d82ac7dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
